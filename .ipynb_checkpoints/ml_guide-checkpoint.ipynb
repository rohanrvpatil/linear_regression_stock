{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382dbeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVING DUPLICATES\n",
    "\n",
    "#Sort values if needed,sorting according to date\n",
    "#Sorting can help group duplicates together, making it easier to identify and analyze them.\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y') #converting object datatype to date datatype\n",
    "cleaned_df=df.sort_values(by=[\"Date\"])\n",
    "cleaned_df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'column1': [1, 2, 2, 3, 4],\n",
    "    'column2': ['A', 'B', 'A', 'C', 'D'],\n",
    "    'column3': ['X', 'Y', 'X', 'Z', 'W']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 1. Identify duplicates based on specific columns\n",
    "duplicates_subset = df[df.duplicated(subset=['column1', 'column2'], keep=False)]\n",
    "print(\"Duplicates based on columns 'column1' and 'column2':\")\n",
    "print(duplicates_subset)\n",
    "\n",
    "# 2. Remove duplicates based on specific columns\n",
    "df.drop_duplicates(subset=['column1', 'column2'], keep='first', inplace=True)\n",
    "print(\"\\nDataFrame after removing duplicates:\")\n",
    "print(df)\n",
    "\n",
    "# 3. Review and confirm results\n",
    "# Observe the modified DataFrame to confirm that duplicates have been removed.\n",
    "\n",
    "# 4. Handle missing values appropriately (optional)\n",
    "# For example, drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 5. Consider case sensitivity\n",
    "# Convert strings to lowercase before identifying duplicates\n",
    "df['column2'] = df['column2'].str.lower()\n",
    "\n",
    "# 6. Backup original data (optional)\n",
    "# Create a copy of the original DataFrame\n",
    "df_backup = df.copy()\n",
    "\n",
    "# 7. Document the process (no code example)\n",
    "Document the steps taken to identify and remove duplicates. This documentation can be helpful for reproducibility and understanding the data cleaning process.\n",
    "\n",
    "# 8. Consider context and business rules (no code example)\n",
    "Take into account the context of your data and any relevant business rules. Sometimes, duplicates may be valid in certain scenarios, and removing them without considering the context could lead to loss of important information.\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(\"\\nFinal DataFrame:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161ee01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZING MEMORY USAGE\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Use Efficient Data Types:\n",
    "# Choose the most memory-efficient data types for your columns. For example, if your numeric columns don't require high precision, consider using `float32` instead of `float64` to save memory.\n",
    "df['column_name'] = df['column_name'].astype('float32')\n",
    "\n",
    "# 2. Downcast Numeric Types:\n",
    "# Downcast numeric columns to lower precision types if possible. This can significantly reduce memory usage.\n",
    "df = df.apply(pd.to_numeric, downcast='float')\n",
    "\n",
    "# 3. Handle Categorical Data Efficiently:\n",
    "# Convert categorical variables to the `category` data type. This can be especially beneficial for columns with a limited number of unique values.\n",
    "df['categorical_column'] = df['categorical_column'].astype('category')\n",
    "\n",
    "# 4. Remove Unnecessary Columns:\n",
    "# Drop columns that are not needed for your analysis. This reduces the overall memory footprint.\n",
    "df = df.drop(['unnecessary_column'], axis=1)\n",
    "\n",
    "# 5. Process Data in Chunks:\n",
    "# If working with large datasets, consider processing data in chunks rather than loading the entire dataset into memory. This is particularly useful when reading data from large files or databases.\n",
    "chunk_size = 10000\n",
    "for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n",
    "    process_chunk(chunk)\n",
    "\n",
    "# 6. Use Sparse Data Structures:\n",
    "# If your data has many zero or missing values, consider using sparse data structures. Pandas supports sparse data types for both Series and DataFrames.\n",
    "df_sparse = df.to_sparse()\n",
    "\n",
    "# 7. Compress Data:\n",
    "# For certain types of data, compression techniques can be applied to reduce memory usage. For example, you can use the `blosc` library or other compression methods.\n",
    "\n",
    "# 8. Optimize String Columns:\n",
    "# If your DataFrame has columns with strings, consider using more memory-efficient representations, such as the `category` data type or storing strings externally.\n",
    "df['string_column'] = df['string_column'].astype('category')\n",
    "\n",
    "# 9. Use External Storage:\n",
    "# For very large datasets that don't fit into memory, consider using external storage solutions like databases or distributed computing frameworks.\n",
    "\n",
    "# These strategies can be applied individually or in combination, depending on the characteristics of your data and the specific requirements of your analysis. Always monitor the impact of these optimizations on the accuracy of your results, as some trade-offs may be involved, especially when downcasting numeric types or using sparse data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113f93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does `df.info()` help in analysing a dataset?\n",
    "\n",
    "The `df.info()` method in pandas is a powerful tool for quickly assessing the structure, composition, and quality of a DataFrame. When analyzing a dataset, the `df.info()` output provides valuable information, including:\n",
    "\n",
    "1. **Number of Rows and Columns:**\n",
    "   - `RangeIndex` and the number of non-null entries in each column give you an immediate sense of the dataset's size.\n",
    "\n",
    "2. **Column Names and Data Types:**\n",
    "   - The list of column names along with their corresponding data types (`int64`, `float64`, `object`, etc.) helps you understand the types of data present in each column.\n",
    "\n",
    "3. **Non-Null Counts:**\n",
    "   - The count of non-null values for each column shows whether there are missing values. Identifying missing data is crucial for data cleaning and imputation strategies.\n",
    "\n",
    "4. **Memory Usage:**\n",
    "   - The memory usage information provides an estimate of the memory consumed by the DataFrame. This can be important for optimizing memory usage, especially when dealing with large datasets.\n",
    "\n",
    "5. **dtype and Memory Usage Optimization:**\n",
    "   - Knowing the data types of columns allows you to optimize memory usage by choosing more memory-efficient types (e.g., using `float32` instead of `float64` for numeric columns).\n",
    "\n",
    "6. **Unique Values (for Object or Categorical Columns):**\n",
    "   - For object or categorical columns, the number of unique values is displayed. This information is useful for understanding the cardinality of categorical variables.\n",
    "\n",
    "7. **Datetime Information (if applicable):**\n",
    "   - If there are datetime columns, their data type and non-null counts are displayed. This is important for time-series analyses.\n",
    "\n",
    "8. **Summary Statistics:**\n",
    "   - If there are numeric columns, summary statistics (count, mean, std, min, 25%, 50%, 75%, max) are provided. These statistics give a quick overview of the distribution of numeric data.\n",
    "\n",
    "By running `df.info()`, you get a concise summary of the dataset, enabling you to make informed decisions about data cleaning, preprocessing, and analysis strategies. It's often one of the first steps in the exploratory data analysis (EDA) process to understand the characteristics and quality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing a Dataset with `df.describe()`\n",
    "\n",
    "## Overview of Numeric Columns\n",
    "\n",
    "- **Count:** The number of non-null entries for each numeric column.\n",
    "- **Mean:** The average value, providing a measure of central tendency.\n",
    "- **Std (Standard Deviation):** A measure of the spread or dispersion of the values around the mean.\n",
    "- **Min:** The minimum value in each column.\n",
    "- **25% (Percentile):** The 25th percentile, or the first quartile, indicating the value below which 25% of the data falls.\n",
    "- **50% (Percentile):** The median, or the 50th percentile, representing the middle value.\n",
    "- **75% (Percentile):** The 75th percentile, or the third quartile, indicating the value below which 75% of the data falls.\n",
    "- **Max:** The maximum value in each column.\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "Identifying Missing Values:identify missing values by comparing the count to the total number of rows.\n",
    "Detecting Constant or Near-Constant Columns: A standard deviation close to zero suggests little variability in the data.\n",
    "Handling Outliers: use the information in the max row to identify potential outliers.\n",
    "\n",
    "- **Count Discrepancies:** Identify columns with count discrepancies, indicating missing values.\n",
    "- **Central Tendency:** Assess the central tendency of the data using the mean and median.\n",
    "- **Data Spread:** Understand the spread or variability of the data through the standard deviation.\n",
    "- **Data Distribution:** Examine the quartiles to understand the distribution of values, identifying potential outliers.\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- **Data Cleaning:** Identify missing values, outliers, or columns with constant values.\n",
    "- **Feature Engineering:** Derive insights for feature scaling, normalization, or transformation.\n",
    "- **Initial Exploration:** Quickly understand the numeric characteristics of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b966c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the categorical and numerical columns. Checking if columns are named something else other than the datatype of values it contains\n",
    "\n",
    "# Categorical columns\n",
    "cat_col = [col for col in cleaned_df.columns if cleaned_df[col].dtype == 'object']\n",
    "print('Categorical columns :',cat_col)\n",
    "# Numerical columns\n",
    "num_col = [col for col in cleaned_df.columns if cleaned_df[col].dtype != 'object']\n",
    "print('Numerical columns :',num_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35acd07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns that don't influence the target\n",
    "df['Ticket'].unique()[:50]\n",
    "\n",
    "Output: #below values are of no use. Feature Engineering might help\n",
    "array(['A/5 21171', 'PC 17599', 'STON/O2. 3101282', '113803', '373450',\n",
    "       '330877', '17463', '349909', '347742', '237736', 'PP 9549',\n",
    "       '113783', 'A/5. 2151', '347082', '350406', '248706', '382652',\n",
    "       '244373', '345763', '2649', '239865', '248698', '330923', '113788',\n",
    "       '347077', '2631', '19950', '330959', '349216', 'PC 17601',\n",
    "       'PC 17569', '335677', 'C.A. 24579', 'PC 17604', '113789', '2677',\n",
    "       'A./5. 2152', '345764', '2651', '7546', '11668', '349253',\n",
    "       'SC/Paris 2123', '330958', 'S.C./A.4. 23567', '370371', '14311',\n",
    "       '2662', '349237', '3101295'], dtype=object)\n",
    "\n",
    "df1 = df.drop(columns=['Name','Ticket'])#removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4befb5ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (1835356475.py, line 18)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 18\u001b[1;36m\u001b[0m\n\u001b[1;33m    1)Dropping observations with missing values.\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "#Handling missing data: \n",
    "round((df1.isnull().sum()/df1.shape[0])*100,2)\n",
    "\n",
    "Output:\n",
    "PassengerId     0.00\n",
    "Survived        0.00\n",
    "Pclass          0.00\n",
    "Sex             0.00\n",
    "Age            19.87\n",
    "SibSp           0.00\n",
    "Parch           0.00\n",
    "Fare            0.00\n",
    "Cabin          77.10\n",
    "Embarked        0.22\n",
    "dtype: float64\n",
    "    \n",
    "The two most common ways to deal with missing data are: \n",
    "1)Dropping observations with missing values.\n",
    "\n",
    "2)The fact that the value was missing may be informative in itself. Plus, in the real world, you often need to make predictions on new data even if some of the features are missing!\n",
    "\n",
    "As we can see from the above result that Cabin has 77% null values and Age has 19.87% and Embarked has 0.22% of null values. So, it’s not a good idea to fill 77% of null values. So, we will drop the Cabin column. Embarked column has only 0.22% of null values so, we drop the null values rows of Embarked column.\n",
    "\n",
    "df2 = df1.drop(columns='Cabin')\n",
    "df2.dropna(subset=['Embarked'], axis=0, inplace=True)\n",
    "df2.shape\n",
    "\n",
    "Imputation is a technique used for replacing the missing data with some substitute value to retain most of the data/information of the dataset\n",
    "\n",
    "Mean imputation is suitable when the data is normally distributed and has no extreme outliers.\n",
    "Median imputation is preferable when the data contains outliers or is skewed.\n",
    "\n",
    "# Mean imputation\n",
    "df3 = df2.fillna(df2.Age.mean())\n",
    "# Let's check the null values again\n",
    "df3.isnull().sum()\n",
    "\n",
    "Output:\n",
    "\n",
    "PassengerId    0\n",
    "Survived       0\n",
    "Pclass         0\n",
    "Sex            0\n",
    "Age            0\n",
    "SibSp          0\n",
    "Parch          0\n",
    "Fare           0\n",
    "Embarked       0\n",
    "dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67f416be",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4201126985.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    Clustering:\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#Handling outliers\n",
    "\n",
    "Clustering:\n",
    "    \n",
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a sample dataset\n",
    "data = {'Feature1': [1, 2, 3, 10, 11, 12, 20, 21, 22],\n",
    "        'Feature2': [2, 3, 4, 15, 16, 17, 25, 26, 27]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Applying DBSCAN\n",
    "eps = 5\n",
    "min_samples = 3\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "df['Cluster'] = dbscan.fit_predict(df)\n",
    "\n",
    "# Extracting outliers (noise points)\n",
    "outliers = df[df['Cluster'] == -1]\n",
    "\n",
    "# Removing outliers from the original DataFrame\n",
    "cleaned_df = df[df['Cluster'] != -1].drop('Cluster', axis=1)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nOutliers:\")\n",
    "print(outliers)\n",
    "print(\"\\nDataFrame after removing outliers:\")\n",
    "print(cleaned_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5149c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Feature1  Feature2  Cluster\n",
      "0         1         2        0\n",
      "1         2         3        0\n",
      "2         3         4        0\n",
      "3        10        15        1\n",
      "4        11        16        1\n",
      "5        12        17        1\n",
      "6        20        25        2\n",
      "7        21        26        2\n",
      "8        22        27        2\n",
      "\n",
      "Outliers:\n",
      "Empty DataFrame\n",
      "Columns: [Feature1, Feature2, Cluster]\n",
      "Index: []\n",
      "\n",
      "DataFrame after removing outliers:\n",
      "   Feature1  Feature2\n",
      "0         1         2\n",
      "1         2         3\n",
      "2         3         4\n",
      "3        10        15\n",
      "4        11        16\n",
      "5        12        17\n",
      "6        20        25\n",
      "7        21        26\n",
      "8        22        27\n"
     ]
    }
   ],
   "source": [
    "# Removing outliers using DBSCAN clustering\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a sample dataset\n",
    "data = {'Feature1': [1, 2, 3, 10, 11, 12, 20, 21, 22],\n",
    "        'Feature2': [2, 3, 4, 15, 16, 17, 25, 26, 106]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Applying DBSCAN\n",
    "eps = 5\n",
    "min_samples = 3\n",
    "dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "df['Cluster'] = dbscan.fit_predict(df)\n",
    "\n",
    "# Extracting outliers (noise points)\n",
    "outliers = df[df['Cluster'] == -1]\n",
    "\n",
    "# Removing outliers from the original DataFrame\n",
    "cleaned_df = df[df['Cluster'] != -1].drop('Cluster', axis=1)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(\"\\nOutliers:\")\n",
    "print(outliers)\n",
    "print(\"\\nDataFrame after removing outliers:\")\n",
    "print(cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc43ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
